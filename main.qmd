---
title: "Optiver"
format: pdf
editor: visual
---

# Optiver Challenge

Installing packages

```{r}
need <- c("httr", "jsonlite", "magrittr", "readr", "openxlsx", "dplyr", "lubridate", "stringr", "purrr", "readxl", "tidyverse") # list packages needed
have <- need %in% rownames(installed.packages()) # checks packages you have
if(any(!have)) install.packages(need[!have]) # install missing packages
invisible(lapply(need, library, character.only=T)) # load needed packages
```

Function to get data

```{r}
get_series_data <- function(series_id, api_key) {
  # Initialize an empty data frame to store all the data for the current series
  all_data <- data.frame()
  
  page <- 1  # Start at page 1
  has_more_data <- TRUE
  
  while (has_more_data) {
    # Define the query parameters (adding observation_start for 2005)
    params <- list(
      series_id = series_id,
      api_key = api_key,
      file_type = "json",
      limit = 10000,  # Reasonable limit per request
      page = page,
      observation_start = "2006-01-01"  # <-- Only get data from Jan 2005 onwards
    )
    
    # Make the API request
    response <- GET(url, query = params)
    
    # Print the HTTP status code for debugging
    cat("Status code for series", series_id, ":", http_status(response)$status_code, "\n")
    
    # Check if the request was successful
    if (http_status(response)$category == "Success") {
      # Parse the JSON response
      data <- content(response, "text") %>% fromJSON()
      
      # Extract the observations
      observations <- data$observations
      
      # Append the new data to all_data
      if (length(observations) > 0) {
        all_data <- rbind(all_data, as.data.frame(observations))
      }
      
      # Pagination: Continue if FRED API provides "next_offset"
      has_more_data <- "next_offset" %in% names(data)
      page <- page + 1
    } else {
      # Print the response content if the request failed for debugging
      print(content(response, "text"))
      stop("Failed to retrieve data from FRED API. Status code:", http_status(response)$status_code)
    }
  }
  
  return(all_data)
}

```

Which datasets we want

```{r}
# Set up the API URL
url <- "https://api.stlouisfed.org/fred/series/observations"

# Define a dictionary mapping series IDs to names
series_names <- list(
  "APU0000708111" = "Average Price: Eggs, Grade A, Large (Cost per Dozen) in U.S. City Average",
  "APU0000709112" = "Average Price: Milk, Fresh, Whole, Fortified (Cost per Gallon/3.8 Liters) in U.S. City Average",
  "WPU01830131" = "Producer Price Index by Commodity: Farm Products: Soybeans",
  "WPU02930102" = "Producer Price Index by Commodity: Processed Foods and Feeds: Chicken and Turkey Feed, Supplements,          Concentrates, and Premixes",
  "WPS012202" = "Producer Price Index by Commodity: Farm Products: Corn",
  "UNRATE" = "Unemployment Rate",
  "UMCSENT" = "University of Michigan: Consumer Sentiment",
  "PCE" = "Personal Consumption Expenditures",
  "MRTSSM7225USN" = "Retail Sales: Restaurants and Other Eating Places",
  "MRTSSM44X72USS" = "Retail Sales: Retail Trade and Food Services",
  "GASREGW" = "US Regular All Formulations Gas Price",
  "GASDESW" = "US Diesel Sales Price",
  "FEDFUNDS" = "Federal Funds Effective Rate", 
  "DSPIC96" = "Real Disposable Personal Income",
  "CUSR0000SEHF" = "Consumer Price Index for All Urban Consumers: Energy Services in U.S. City Average",
  "CPIUFDSL" = "Consumer Price Index for All Urban Consumers: Food in U.S. City Average",
  "CPIAUCSL" = "Consumer Price Index for All Urban Consumers: All Items in U.S. City Average", 
  "APU0000FF1101" = "Average Price: Chicken Breast, Boneless (Cost per Pound/453.6 Grams) in U.S. City Average",
  "APU0000706111" = "Average Price: Chicken, Fresh, Whole (Cost per Pound/453.6 Grams) in U.S. City Average"
)

# Define the list of series IDs you want to load
series_ids <- names(series_names)

# Your FRED API key
secrets <- fromJSON("./secrets.json")
api_key <- secrets$API_key
```

Create folders and directories

```{r}
# some setup: pathing and folder structure
table_dir <- "./output/tables/"
figure_dir <- "./output/figures/"
data_folder <- "./data/raw/"
output_file <- "./data/processed/merged_dataset.xlsx"  # Output file
dir.create(table_dir, recursive = TRUE)
dir.create(figure_dir, recursive = TRUE)
dir.create(data_folder, recursive = TRUE)
dir.create("./data/processed")

# Directory to save the CSV file
setwd("~/Optiver-Challenge")
```

Loop and save data outputs

```{r}
for (series_id in series_ids) {
  series_name <- series_names[[series_id]]  
  cat("Retrieving data for series:", series_name, "(", series_id, ")\n")
  
  # Slow down API calls (to stay under 120 requests per minute)
  Sys.sleep(1)  # Ensures max ~60 requests/min (adjust this if needed)

  # Retry logic in case of rate limit (HTTP 429)
  retry <- TRUE
  attempts <- 0
  max_attempts <- 5  # Limit retries to prevent infinite loops
  
  while (retry && attempts < max_attempts) {
    attempts <- attempts + 1
    
    series_data <- tryCatch({
      get_series_data(series_id, api_key)
    }, error = function(e) {
      cat("Error retrieving data for series:", series_id, ":", e$message, "\n")
      NULL
    })
    
    # If the request fails due to rate limits (429), wait and retry
    if (is.null(series_data)) {
      cat("API rate limit exceeded. Waiting for 30 seconds before retrying...\n")
      Sys.sleep(30)  # Increase delay when hitting rate limit
    } else {
      retry <- FALSE  # If data is retrieved successfully, stop retrying
    }
  }
  
  # If data was retrieved successfully, save it
  if (!is.null(series_data)) {
    # Sanitize the series_name for valid file naming
    safe_name <- gsub(" ", "_", series_name)  # Replace spaces with underscores
    safe_name <- gsub("[^[:alnum:]_]", "", safe_name)  # Remove any non-alphanumeric characters (except underscores)
    
    # Check if file path is valid and create it
    file_name <- file.path(data_folder, paste0(safe_name, "_data.csv"))
    
    
    # Save the data to an Excel file
    file_name <- file.path(data_folder, paste0(safe_name, "_data.xlsx"))
    write.xlsx(series_data, file_name, rowNames = FALSE)
    cat("Data for", series_name, "saved to", file_name, "\n")
  } else {
    cat("No data retrieved for series:", series_name, "(", series_id, ")\n")
  }
}

cat("All data retrieval and saving complete.\n")

```

Imputing one missing month (average price of chicken fresh) 
```{r}
# Step 1: Read the Excel file
df <- read_excel("~/Optiver-Challenge/data/raw/Average_Price_Chicken_Fresh_Whole_Cost_per_Pound4536_Grams_in_US_City_Average_data.xlsx")

# Step 2: Convert "." to NA in the value column
df$value[df$value == "."] <- NA

# Step 3: Ensure the 'value' column is numeric
df$value <- as.numeric(df$value)

# Step 4: Impute the missing value with the average of the previous and next value
na_index <- which(is.na(df$value))  # Find the index of the missing value (NA)

if (length(na_index) == 1) {
  prev_value <- df$value[na_index - 1]  # Previous value
  next_value <- df$value[na_index + 1]  # Next value
  
  # Impute with the average of the previous and next value
  if (!is.na(prev_value) & !is.na(next_value)) {
    df$value[na_index] <- (prev_value + next_value) / 2
  }
}

# Step 6: Save the modified dataframe as the same Excel file
write.xlsx(df, "~/Optiver-Challenge/data/raw/Average_Price_Chicken_Fresh_Whole_Cost_per_Pound4536_Grams_in_US_City_Average_data.xlsx", overwrite = TRUE)

# print to check the result
print("File saved successfully")
```

merging the datasets 
```{r}
# Function to read, process, and aggregate the datasets
read_and_process <- function(file) {
  # Extract the base column name from the file (clean up name)
  col_name <- basename(file) %>%
    tools::file_path_sans_ext() %>%
    gsub("_data", "", .) %>%
    gsub("[^A-Za-z0-9]", "_", .)

  # Read the Excel file, selecting columns C and D (date and value)
  df <- read_excel(file, col_names = FALSE) %>%
    select(3, 4) %>%
    rename(date = 1, value = 2)

  # Ensure 'date' is in Date format
  df <- df %>% mutate(date = as.Date(date, format = "%Y-%m-%d"))

  # Handle missing values represented by "." or blank ""
  df <- df %>%
    mutate(value = ifelse(value == "." | value == "", NA, value)) %>%
    mutate(value = as.numeric(value))

  # Convert to monthly averages
  monthly_df <- df %>%
    mutate(date = floor_date(date, "month")) %>%
    group_by(date) %>%
    summarise(!!col_name := mean(value, na.rm = TRUE), .groups = "drop")

  return(monthly_df)
}

# List xlsx files
xlsx_files <- list.files(path = "./data/raw", pattern = "^[^~].*\\.xlsx$", full.names = TRUE)

# Process each file into a monthly aggregated dataset
data_list <- map(xlsx_files, read_and_process)

# Merge all datasets by the date column
merged_data <- reduce(data_list, full_join, by = "date") %>%
  arrange(date)

# Step 1: Save the merged dataset (without imputation) to a file
write.xlsx(merged_data, "./data/processed/merged_dataset_without_imputation.xlsx", rowNames = FALSE)
cat("Merged dataset saved to ./data/processed/merged_dataset_without_imputation.xlsx\n")

```

Impute January values
```{r}

# Read the dataset with explicit date handling
merged_data_reopened <- read.xlsx("./data/processed/merged_dataset_without_imputation.xlsx")

# Convert Excel serial date to proper Date format
merged_data_reopened$date <- as.Date(as.numeric(merged_data_reopened$date), origin = "1899-12-30")

# Create a new row for the target date
target_date <- as.Date("2025-01-01")  # Changed to January 1st
new_row <- data.frame(date = target_date)
for(col in colnames(merged_data_reopened)[-1]) {
  new_row[[col]] <- NA
}

# Append the new row
merged_data_reopened <- rbind(merged_data_reopened, new_row)
merged_data_reopened <- merged_data_reopened[order(merged_data_reopened$date), ]

# Get the row index for 2025-01-01
target_row <- which(merged_data_reopened$date == target_date)

# Function to perform linear extrapolation
extrapolate_value <- function(data, column) {
  last_values <- tail(na.omit(data[, column]), 2)
  if (length(last_values) >= 2) {
    return(last_values[2] + (last_values[2] - last_values[1]))
  }
  return(NA)
}

# Create a data frame to store the results
imputed_values <- data.frame(Column = character(), Value = numeric(), stringsAsFactors = FALSE)

# Process each column except date and store results
for (col in colnames(merged_data_reopened)[-1]) {
  value <- extrapolate_value(merged_data_reopened, col)
  merged_data_reopened[target_row, col] <- value
  imputed_values <- rbind(imputed_values, data.frame(Column = col, Value = value))
}

# Display imputed values in a readable format
print("\nImputed values for 2025-01-01:")
for (i in 1:nrow(imputed_values)) {
  cat(sprintf("\n%s: %.3f", imputed_values$Column[i], imputed_values$Value[i]))
}

# Save the updated dataset
write.xlsx(merged_data_reopened, "./data/processed/merged_dataset_imputed.xlsx", rowNames = FALSE)

# Verify the save was successful
cat("\n\nDataset has been saved to: ./data/processed/merged_dataset_imputed.xlsx")
 
```


